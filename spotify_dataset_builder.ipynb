{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 1 - Analyzing and preparing the charts:**\n",
    "##### **1.1 - Importing packages and modules:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import math\n",
    "from prettytable import PrettyTable\n",
    "import creds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.2 - Analyzing and cleaning the charts:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from files in week_charts folder:\n",
    "\n",
    "# Getting the files' names from \\week_charts folder and creating a list for the files' analysis:\n",
    "files_names = os.listdir('charts')\n",
    "generic_problems = []\n",
    "specific_problems = []\n",
    "\n",
    "# Loop through the names and reading the .csv files:\n",
    "for file in files_names:\n",
    "    path = 'charts/' + str(file)\n",
    "    chart = pd.read_csv(path)\n",
    "    \n",
    "    # Checking for generic problems with respect to charts (missing and duplicated rows):\n",
    "    if (len(chart) != 200) or len(chart[chart.duplicated() == True]):\n",
    "        name = str(file)\n",
    "        missing_rows = '-----' if (200 - len(chart)) == 0 else (200 - len(chart))\n",
    "        duplicated_rows = '-----' if len(chart[chart.duplicated() == True]) == 0 else len(chart[chart.duplicated() == True])\n",
    "        generic_problem = [name, missing_rows, duplicated_rows]\n",
    "        generic_problems.append(generic_problem)\n",
    "        \n",
    "    # Checking for specific problems with charts' columns:\n",
    "    \n",
    "    if (len(chart[chart['uri'].isnull()]) > 0 or len(chart[chart['uri'] == '']) > 0 or\n",
    "        len(chart[chart['uri'].duplicated()]) > 0 or\n",
    "        len(chart[chart['artist_names'].isnull()]) > 0 or len(chart[chart['artist_names'] == '']) > 0 or\n",
    "        len(chart[chart['track_name'].isnull()]) > 0 or len(chart[chart['track_name'] == '']) > 0 or\n",
    "        len(chart[chart['source'].isnull()]) > 0 or len(chart[chart['source'] == '']) > 0 or\n",
    "        len(chart[chart['weeks_on_chart'].isnull()]) or\n",
    "        min(chart['weeks_on_chart']) <= 0 or\n",
    "        len(chart[chart['streams'].isnull()]) or\n",
    "        min(chart['streams']) <= 0):\n",
    "\n",
    "        name = str(file)\n",
    "        missing_uri = ('-----' if (len(chart[chart['uri'].isnull()]) == 0 and len(chart[chart['uri'] == '']) == 0) \n",
    "                       else (len(chart[chart['uri'].isnull()]) + len(chart[chart['uri'] == ''])))                \n",
    "        duplicated_uri = ('-----' if len(chart[chart['uri'].duplicated()]) == 0 else len(chart[chart['uri'].duplicated()]))        \n",
    "        missing_artist_name = ('-----' if (len(chart[chart['artist_names'].isnull()]) == 0 and len(chart[chart['artist_names'] == '']) == 0) \n",
    "                       else (len(chart[chart['artist_names'].isnull()]) + len(chart[chart['artist_names'] == ''])))        \n",
    "        missing_track_name = ('-----' if (len(chart[chart['track_name'].isnull()]) == 0 and len(chart[chart['track_name'] == '']) == 0) \n",
    "                       else (len(chart[chart['track_name'].isnull()]) + len(chart[chart['track_name'] == ''])))        \n",
    "        missing_source = ('-----' if (len(chart[chart['source'].isnull()]) == 0 and len(chart[chart['source'] == '']) == 0) \n",
    "                       else (len(chart[chart['source'].isnull()]) + len(chart[chart['source'] == ''])))        \n",
    "        missing_weeks_on_charts = ('-----' if (len(chart[chart['weeks_on_chart'].isnull()]) == 0) \n",
    "                       else (len(chart[chart['weeks_on_chart'].isnull()])))        \n",
    "        inconsistent_weeks_on_chart = ('-----' if (len(chart[chart['weeks_on_chart'] <= 0]) <= 0)\n",
    "                                       else (len(chart[chart['weeks_on_chart'] <= 1])))        \n",
    "        missing_streams = ('-----' if (len(chart[chart['streams'].isnull()]) == 0) \n",
    "                       else (len(chart[chart['streams'].isnull()])))        \n",
    "        inconsistent_streams = ('-----' if (len(chart[chart['streams'] <= 0]) <= 0)\n",
    "                                else (len(chart[chart['streams'] <= 1])))\n",
    "        \n",
    "        specific_problem = [name, missing_uri, duplicated_uri, missing_artist_name, missing_track_name, duplicated_track_name,\n",
    "                            missing_source, missing_weeks_on_charts, inconsistent_weeks_on_chart, missing_streams, inconsistent_streams]\n",
    "        specific_problems.append(specific_problem)    \n",
    "    \n",
    "# Printing the generic problems with the charts:\n",
    "if len(generic_problems) != 0:\n",
    "    gen_pro_table = PrettyTable(['Files Names', 'Missing Rows', \"Duplicated Rows\"])\n",
    "    gen_pro_table.title = \"Files with Generic Problems\"\n",
    "    for problem in generic_problems:\n",
    "        gen_pro_table.add_row(problem)\n",
    "    print(gen_pro_table)\n",
    "else:\n",
    "    print('There are no generic problems (missing or dumplicated rows) with the charts downloaded.')\n",
    "    \n",
    "# Printing the specific problems with the charts:\n",
    "if len(specific_problems) != 0:\n",
    "    spe_pro_table = PrettyTable(['Files Names', 'Missing uri', 'Duplicated uri', 'Missing artist_names', 'Missing track_name',\n",
    "                                 'Missing source', 'Missing weeks_on_chart', 'Inconsistent weeks_on_chart',\n",
    "                                 'Missing streams', 'Inconsistent streams'])\n",
    "    spe_pro_table.title = \"Files with Specific Problems\"\n",
    "    for problem in specific_problems:\n",
    "        spe_pro_table.add_row(problem)\n",
    "    print(spe_pro_table)\n",
    "else:\n",
    "    print(\"\\nThere are no charts with specific problems in its columns' values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.3 - Removing unwanted columns, joining the charts and organizing the dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the charts in one single dataframe:\n",
    "\n",
    "merged_charts_df = pd.DataFrame()\n",
    "\n",
    "for file in files_names:\n",
    "    path = 'charts/' + str(file)\n",
    "    new_df = pd.read_csv(path)\n",
    "    new_df = new_df.drop(['rank', 'peak_rank', 'previous_rank'], axis=1)\n",
    "    merged_charts_df = pd.concat([merged_charts_df, new_df])\n",
    "\n",
    "# Grouping the rows by 'uri', summing the streams and getting the max from 'weeks_on_chart' column:\n",
    "\n",
    "merged_charts_df = merged_charts_df.groupby('uri', as_index=False).agg({'artist_names': 'first', 'track_name': 'first',\n",
    "                                                                        'source': 'first', 'weeks_on_chart': 'max',\n",
    "                                                                        'streams': 'sum'})\n",
    "\n",
    "# Getting the tracks ids from 'uri' and renaming the column to 'id':\n",
    "\n",
    "merged_charts_df['uri'] = merged_charts_df['uri'].str.replace('spotify:track:','')\n",
    "merged_charts_df.rename(columns = {'uri':'id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 2 - Using Spotify's API to get the audio features of the tracks:**\n",
    "\n",
    "##### **2.1 - Getting access to Spotify's API service:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting access to Spotify's API service:\n",
    "\n",
    "# API information:\n",
    "client_id = creds.client_id\n",
    "client_secret = creds.client_secret\n",
    "\n",
    "# Getting an access token:\n",
    "AUTH_URL = 'https://accounts.spotify.com/api/token'\n",
    "auth_response = requests.post(AUTH_URL, {\n",
    "    'grant_type': 'client_credentials',\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "})\n",
    "auth_response_data = auth_response.json()\n",
    "access_token = auth_response_data['access_token']\n",
    "headers = {\n",
    "    'Authorization': 'Bearer {token}'.format(token=access_token)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.2 - Using the tracks ids to request information about the audio features of the tracks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering the 'ids' to request more information from Spotify's API:\n",
    "\n",
    "tracks_id = merged_charts_df['id'].values.tolist()\n",
    "\n",
    "# Getting information about the tracks' audio features from Spotify's API service:\n",
    "\n",
    "BASE_URL = 'https://api.spotify.com/v1/audio-features?ids='\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i in range(math.ceil(len(tracks_id) / 100)):\n",
    "    start_int = i*100\n",
    "    end_int = start_int + 100\n",
    "    ids = \",\".join(tracks_id[(start_int):(end_int)])\n",
    "    r = requests.get(BASE_URL + ids, headers=headers)\n",
    "    responses.append(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.3 - Creating a dataframe to store the information from Spotify's API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all the information from Spotify's API in a new dataframe:\n",
    "\n",
    "list_of_features = []\n",
    "\n",
    "for response in responses:\n",
    "    res = list(next(iter(response.values())))\n",
    "    for re in res:\n",
    "        values  = list(re.values())\n",
    "        list_of_features.append(values)\n",
    "        \n",
    "audio_features_df = pd.DataFrame(columns=re.keys())\n",
    "for item in list_of_features:\n",
    "    audio_features_df.loc[len(audio_features_df)] = item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 3 - Joining the two dataframes (charts and audio features), organizing and creating the .csv file:**\n",
    "\n",
    "##### **3.1 - Joining the two dataframes (charts and audio features)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the new dataframe with the merged_charts_df:\n",
    "\n",
    "top_songs_audio_features_df = pd.merge(merged_charts_df, audio_features_df, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3.2 - Organizing the dataframe: converting values, removing and rearranging columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unwanted information from the dataframe: \n",
    "\n",
    "top_songs_audio_features_df = top_songs_audio_features_df.drop(['type', 'uri', 'track_href', 'analysis_url'], axis=1)\n",
    "\n",
    "# Rearranging the columns in accordance to the nature of the data:\n",
    "\n",
    "top_songs_audio_features_df = top_songs_audio_features_df[['id', 'artist_names', 'track_name','source', 'key', 'mode', \n",
    "                                                           'time_signature', 'danceability', 'energy', 'speechiness', 'acousticness',\n",
    "                                                           'instrumentalness', 'liveness', 'valence', 'loudness', 'tempo',\n",
    "                                                           'duration_ms', 'weeks_on_chart', 'streams']]\n",
    "# Convert keys' numbers to keys' names:\n",
    "top_songs_audio_features_df['key'] = top_songs_audio_features_df['key'].replace([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \n",
    "                                                                  [\"No key detected\", \"C\",\"C#/Db\",\"D\",\"D#/Eb\",\n",
    "                                                                   \"E\",\"F\",\"F#/Gb\",\"G\",\"G#/Ab\",\"A\",\"A#/Bb\",\"B\"])\n",
    "\n",
    "# Convert modes' numbers to modes' names:\n",
    "top_songs_audio_features_df['mode'] = top_songs_audio_features_df['mode'].replace([0, 1], ['Minor', 'Major'])\n",
    "\n",
    "# Convert time_signature number to names:\n",
    "top_songs_audio_features_df['time_signature'] = (top_songs_audio_features_df['time_signature'].replace\n",
    "                                                 ([3, 4, 5, 6, 7], ['3 beats', '4 beats', '5 beats', '6 beats', '7 beats']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3.3 - Creating the .csv file with all the information:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe into a .csv file:\n",
    "\n",
    "top_songs_audio_features_df.to_csv('spotify_top_songs_audio_features.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
